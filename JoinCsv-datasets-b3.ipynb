{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Junta os CSV\n",
    "\n",
    "Junta todos os csv de um determinado diretório em um único csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_csv(pathFrom, fileTo, field='Fechamento', fielddate='Data', limit=0, invert=False, formatdatedefault=''):\n",
    "    counter = 0\n",
    "    newCsv = pd.DataFrame()\n",
    "\n",
    "    for file in os.listdir(pathFrom):\n",
    "        counter = counter+1\n",
    "        csv = pd.read_csv(pathFrom+'/'+file)\n",
    "        \n",
    "        # first column is Data\n",
    "        if(counter == 1):\n",
    "            if (formatdatedefault == ''):\n",
    "                newCsv['Data'] = csv[fielddate]\n",
    "            else:\n",
    "                newCsv['Data'] = pd.to_datetime(csv[fielddate], format=formatdatedefault).dt.strftime('%d/%m/%Y')\n",
    "\n",
    "        name = os.path.splitext(file)[0]\n",
    "        name = name.replace('.SAO', '')\n",
    "        newCsv[name] = csv[field]\n",
    "    \n",
    "    # inverte os dados: do mais antigo para o mais novo\n",
    "    if (invert):\n",
    "        newCsv = newCsv.iloc[::-1].copy()\n",
    "        newCsv.reset_index(drop=True, inplace=True)\n",
    "            \n",
    "    if (limit > 0):\n",
    "        # orderna do mais recente para o mais antigo\n",
    "        pos = newCsv.shape[0]-limit\n",
    "        newCsv = newCsv[pos:]\n",
    "\n",
    "    newCsv.to_csv(fileTo, index=False)\n",
    "    print('New file CSV successfully created')\n",
    "    \n",
    "def find_csv_max_lines(path):\n",
    "    rows = []\n",
    "    for file in os.listdir(path):\n",
    "        with open(path+'/'+file) as f:\n",
    "            lines = sum(1 for line in f)\n",
    "            rows.append([file, lines])\n",
    "\n",
    "    csv = pd.DataFrame(rows, columns=['File', 'Lines'])\n",
    "    maxlines = csv['Lines'].max()\n",
    "    return csv.loc[(csv['Lines'] == maxlines)]\n",
    "\n",
    "def join_csv_merge(dfcsv, fileTo, path_old, path_older, list_choice=[], limit=0):\n",
    "    newCsv = pd.DataFrame()\n",
    "    define_data = False\n",
    "    lenght = len(list_choice)\n",
    "    \n",
    "    for i, row_merge in merge.iterrows():\n",
    "        name = os.path.splitext(row_merge.File)[0]\n",
    "        if (lenght > 0 and name not in list_choice):\n",
    "            continue\n",
    "            \n",
    "        fe1 = pd.read_csv(path_older+'/'+row_merge.File)\n",
    "        fe2 = pd.read_csv(path_old+'/'+row_merge.File)\n",
    "        dates=[]\n",
    "\n",
    "        if (define_data == False):\n",
    "            for i, row in fe1.iterrows():\n",
    "                dates.append(row['Data'])\n",
    "            for i, row in fe2.iterrows():\n",
    "                dates.append(row['Data'])\n",
    "            newCsv['Data'] = dates\n",
    "            define_data = True\n",
    "\n",
    "        rows=[]\n",
    "        for i, row in fe1.iterrows():\n",
    "            rows.append(row['Fechamento'])\n",
    "        for i, row in fe2.iterrows():\n",
    "            rows.append(row['Fechamento'])\n",
    "        \n",
    "        newCsv[name] = rows\n",
    "    \n",
    "    if (limit > 0):\n",
    "        # orderna do mais recente para o mais antigo\n",
    "        pos = newCsv.shape[0]-limit\n",
    "        newCsv = newCsv[pos:]\n",
    "    newCsv.to_csv(fileTo,index=False)\n",
    "\n",
    "    print('New file CSV successfully created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# De 1 diretório\n",
    "#### Usando a função para juntar os csv's de 'datasets-b3' em um único arquivo com o fechamento de todos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New file CSV successfully created\n"
     ]
    }
   ],
   "source": [
    "join_csv('datasets-b3', 'datasets/data.csv', limit=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usando a função para juntar os csv's de 'datasets-alpha'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New file CSV successfully created\n"
     ]
    }
   ],
   "source": [
    "path_data_alpha = 'datasets/data_alpha.csv'\n",
    "join_csv('historic-alpha vantage/datasets-treated', path_data_alpha, field='4. close', fielddate='date', limit=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_alpha = pd.read_csv(path_data_alpha)\n",
    "test=[]\n",
    "for col in data_alpha.columns:\n",
    "    su = data_alpha[col].isnull().sum()\n",
    "    if(su > 0):\n",
    "        test.append([col, su])\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_alpha['NTCO3']\n",
    "data_alpha.to_csv(path_data_alpha, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# De 2 diretórios\n",
    "#### Junta o preço de Fechamento de todos os csv de 2 diretório em um único csv, depois organiza todo os fechamentos em um csv\n",
    "Somente os csv que existem nos dois diretórios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((262, 2), (204, 2))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv19 = find_csv_max_lines('datasets-b3-2019')\n",
    "csv20 = find_csv_max_lines('datasets-b3')\n",
    "csv20.shape, csv19.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge = pd.merge(csv20, csv19, left_on='File', right_on='File')\n",
    "merge.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### De todas as ações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join_csv_merge(merge, 'datasets/data.csv', 'datasets-b3', 'datasets-b3-2019', limit=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Somente de algumas ações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     ABEV3\n",
       "1     AZUL4\n",
       "2     B3SA3\n",
       "3     BBAS3\n",
       "4     BBDC3\n",
       "      ...  \n",
       "70    VALE3\n",
       "71    VIVT4\n",
       "72    VVAR3\n",
       "73    WEGE3\n",
       "74    YDUQ3\n",
       "Name: Código, Length: 75, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tickers = pd.read_csv('datasets/composicao_indice.csv', names=['Código'])['Código']\n",
    "tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New file CSV successfully created\n"
     ]
    }
   ],
   "source": [
    "join_csv_merge(merge, 'datasets/data_cart.csv', 'datasets-b3', 'datasets-b3-2019', tickers.tolist(), limit=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((300, 71), 'from 12/04/2019 to 26/06/2020')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('datasets/data_cart.csv', index_col=0)\n",
    "data.shape, 'from {} to {}'.format(data.index[0], data.index[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
